{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "designed-titanium",
   "metadata": {},
   "source": [
    "# Task 2: a more challenging percolation problem\n",
    "\n",
    "Mention periodic boundary conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entitled-situation",
   "metadata": {},
   "source": [
    "## Running this variation of the model\n",
    "\n",
    "### Importing the code\n",
    "\n",
    "Of course, the first thing we need to do is import the relevant bits of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chronic-creation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from p1b_percolation.lattice import SquareLattice\n",
    "from p1b_percolation.model import PercolationModel\n",
    "from p1b_percolation.scripts.parameter_scan import parameter_scan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "american-accuracy",
   "metadata": {},
   "source": [
    "### Setting up the model\n",
    "\n",
    "We're going to provide a second argument to `SquareLattice`, called `n_links`.\n",
    "In the previous notebook, `n_links` took its default value of 1, but we now want to change it to 3.\n",
    "\n",
    "So, the way we will create a $50\\times 50$ lattice is\n",
    "```python\n",
    "lattice = SquareLattice(50, 50, n_links=3)\n",
    "```\n",
    "\n",
    "You might wonder why we don't just write\n",
    "```python\n",
    "lattice = SquareLattice(50, 50, 3)\n",
    "```\n",
    "Python allows both, but it is usually preferable to explicitly state the name of the argument which you are providing a value for.\n",
    "This is particularly helpful when a function has many arguments, and you don't want to rely on remembering the order in which they are supposed to appear.\n",
    "\n",
    "We could have been even more explicit and written\n",
    "```python\n",
    "lattice = SquareLattice(n_rows=50, n_cols=50, n_links=3)\n",
    "```\n",
    "but it's easy to remember that the first two arguments specify the size of the lattice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processed-cookie",
   "metadata": {},
   "outputs": [],
   "source": [
    "lattice = SquareLattice(50, 50, n_links=3)\n",
    "model = PercolationModel(\n",
    "    lattice,\n",
    "    frozen_prob=0.4,\n",
    "    recovery_time=-1,\n",
    ")\n",
    "\n",
    "model.animate(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "labeled-queue",
   "metadata": {},
   "source": [
    "## Measuring the percolation transition\n",
    "\n",
    "To find the percolation transition, we will use a method similar to one you saw in the previous notebook.\n",
    "We will run a number, `repeats`, of separate simulations and record the fraction of these that percolated,\n",
    "\n",
    "$$\n",
    "f = \\frac{1}{N} \\sum_{i=1}^N b_i \\tag{Estimator}\n",
    "$$\n",
    "\n",
    "where $b_i$ are equal to either 0 (for simulations which don't percolate) or 1 (for those that do), and $N$ is the number of simulations (`repeats`).\n",
    "\n",
    "### Errors\n",
    "\n",
    "Just as before, $N \\times f$ still follows a binomial distribution, since it is the sum of binary variables (the result of 'Bernoulli trials').\n",
    "Thus, the formula for the standard error on $f$ remains the same:\n",
    "\n",
    "$$\n",
    "\\sigma_f = \\sqrt{\\frac{p(1 - p)}{N}} \\tag{Standard error}\n",
    "$$\n",
    "\n",
    "Previously, we calculated $p$ by hand using basic probability theory.\n",
    "Unfortunately, in this (and almost every interesting) situation, we can't just calculate the answer by hand, otherwise we wouldn't bother spending all this effort estimating it using simulations!\n",
    "\n",
    "The standard approach in situations like this is to replace $p$ with its estimate, which is $f$.\n",
    "\n",
    "The cell below also calculates an estimate of the standard error using $f$ in place of $p$.\n",
    "Run the cell for a few values of `frozen_prob`, including some for which all, or none, of the simulations percolate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "failing-citation",
   "metadata": {},
   "outputs": [],
   "source": [
    "frozen_prob = 0.1  # change this value!\n",
    "repeats = 100  # and this one!\n",
    "\n",
    "lattice = SquareLattice(50, 50, n_links=3)\n",
    "model = PercolationModel(lattice, frozen_prob, recovery_time=-1)\n",
    "\n",
    "model.estimate_percolation_prob(repeats, print_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sweet-assumption",
   "metadata": {},
   "source": [
    "You should see that the estimate of the standard error can be zero, when $f$ is either 0 or 1.\n",
    "Discuss this observation.\n",
    "\n",
    "This example illustrates why it is so important to distinguish between the estimator, $f$ (which is only as good as your experiment!), and the thing being estimated, $p$.\n",
    "Of course, it is not correct to conclude that the error is zero when $f=0$ or $f=1$; we cannot say with 100\\% certainty whether the next simulation will percolate or not.\n",
    "\n",
    "However, as we increase $N$, $f$ will become a better and better estimate of $p$, and in turn the estimate of the error will improve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "three-harvard",
   "metadata": {},
   "source": [
    "### Find the percolation transition\n",
    "\n",
    "\n",
    "\n",
    "We will attempt to fit a **Logistic** function\n",
    "\n",
    "$$\n",
    "S(q) = \\frac{1}{1 + e^{\\lambda (q - q_0)}}\n",
    "$$\n",
    "\n",
    "using the *method of least squares*.\n",
    "\n",
    "$S(q)$ is a very simple function that smoothly transitions between the asymptotes\n",
    "\n",
    "$$\n",
    "\\lim_{q\\to-\\infty} S(q) = 1 \\qquad\\qquad\n",
    "\\lim_{q\\to\\infty} S(q) = 0\n",
    "$$\n",
    "\n",
    "and has two *free parameters*:\n",
    "* The mid-point $q_0$, such that $S(q_0) = \\frac{1}{2}$\n",
    "* The 'steepness' $\\lambda$, which controls how quickly the transition from approximately 1 to approximately 0 occurs\n",
    "\n",
    "In this sense it is comparable to a linear fit, which also has two free parameters - the gradient and the y-intercept.\n",
    "Interested students can take 5 minutes at this point to have a read through the section [Further reading: Fitting functions to data](#Further-reading:-Fitting-functions-to-data) before returning to this point.\n",
    "\n",
    "The advantage of using the Logistic function to fit the data here is that it will perform better than the linear fit when we take a larger range of values, meaning we don't have to first expend lots of effort in finding the values of `frozen_prob` which give $f\\approx \\frac{1}{2}$ and over which the trend is approximately linear.\n",
    "\n",
    "However, it is important to realise that we do not have a *theory* telling us that $S(q)$ is an appropriate thing to fit.\n",
    "\n",
    "Run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structured-insulin",
   "metadata": {},
   "outputs": [],
   "source": [
    "lattice = SquareLattice(50, 50, n_links=3)\n",
    "model = PercolationModel(lattice, recovery_time=-1)\n",
    "\n",
    "parameter_scan(model, start=0.35, stop=0.45, num=50, repeats=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "short-chrome",
   "metadata": {},
   "source": [
    "Comment on the quality of fit.\n",
    "Do the residuals show an obvious structure?\n",
    "\n",
    "## Extrapolating to infinite lattice size\n",
    "\n",
    "There is one glaring issue with all the analysis we have done so far; all of our results depend on the size of the lattice!\n",
    "\n",
    "So-called *finite-volume effects* are an inevitability with simulations, since we cannot simulate an infinite system.\n",
    "However, it is possible to *extrapolate* results to the infinite limit, by looking at how the thing you're measuring varies as the size of simulations increases.\n",
    "The answers you get after extrapolating are more likely to agree with the physical world.\n",
    "\n",
    "We will attempt to extract some infinite-lattice results for where the percolation transition occurs ($q_0$).\n",
    "\n",
    "You will need to run the cell below for a range of different values of `n_rows`, the number of rows (the number of columns will automatically be set equal to this).\n",
    "Let's label the number of rows as $L$.\n",
    "\n",
    "Record your results in a table, making sure to note down the associated errors on $q_0$ **with an appropriate precision**.\n",
    "Also record your values for `start` and `stop` in the table; these will need to be carefully selected for each lattice size so that almost all of the data points take values between 0 and 1, but are not 0 or 1.\n",
    "**Leave three blank columns in your table.**\n",
    "For example, your table could look like this\n",
    "\n",
    "| Lattice size ($L$, `n_rows`) | Min $q$ (`start`) | Max $q$ (`stop`) | Mid point ($q_0$) | Error ($\\sigma_{q_0}$) | - | - | - |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| ... | ... | ... | ... | ... | ... | ... | ... |\n",
    "\n",
    "Keep a note of what you used for`num` and `repeats`, but since there is no need to vary them they do not require a column in the table.\n",
    "\n",
    "Aim to gather results for 5-10 different lattice sizes ranging between `n_rows = 20` and `n_rows = 200`.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Warning!</b> \n",
    "    The larger simulations will take a while to complete. There is no need to generate more than 25 data points for each lattice size, and 25 repeats should also suffice. The largest lattice size may take up to 5 minutes, so grab a cup of coffee while you wait!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hearing-review",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = 20  # change this!\n",
    "n_cols = n_rows\n",
    "\n",
    "lattice = SquareLattice(n_rows, n_cols, n_links=3)\n",
    "model = PercolationModel(lattice, recovery_time=-1)\n",
    "\n",
    "parameter_scan(model, start=0.3, stop=0.5, num=35, repeats=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uniform-disease",
   "metadata": {},
   "source": [
    "You may be able to tell, by just studying the numbers in your table, that the relationship between lattice size (`n_rows`) and the mid-point of the transition is not linear.\n",
    "\n",
    "We really can't say *a priori* what the relationship is, but it's often a good next step to take logarithms to check if the relationship is a power-law.\n",
    "\n",
    "In the remaining three columns of your table, calculate the logarithm of the lattice size, the logarithm of $q_0$, and the error on $\\log q_0$.\n",
    "This will require you to *propagate* the error on $q_0$ through the logarithm.\n",
    "Refer to the lab manual if you need to double check how to do this.\n",
    "\n",
    "Plot a graph of $\\log L$ v.s. $\\log q_0(L)$ and draw a line of best fit.\n",
    "Calculate the gradient of your line, with an estimate of its error.\n",
    "\n",
    "Now, let's say we take the lattice size to tend towards infinity.\n",
    "What does your graph predict as the position of the percolation transition for an infinite lattice, $q_0(\\infty)$?\n",
    "Can you explain this result?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "front-arizona",
   "metadata": {},
   "source": [
    "This cell is left intentionally blank."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "choice-vienna",
   "metadata": {},
   "source": [
    "# Further reading: Fitting functions to data\n",
    "\n",
    "> Note: Instead of talking of fitting *functions* with *arguments* to data, it is more common to speak of fitting a *model* with *free parameters* to some data.\n",
    "\n",
    "Throughout your time at school or college you may have spent a good deal of time drawing straight lines through data on paper, or perhaps even using something like Excel.\n",
    "\n",
    "*Linear fits* are exceptionally useful, and as such, if it is possible to manipulate your data into a form where it is fit well by a straight line, then this is the approach you should take.\n",
    "As an example, we frequently take the logarithm of our variables if they obey a power law $y = A x^\\alpha$, since the variables $\\log y$ and $\\log x$ are related by the *linear* equation $\\log y = \\alpha \\log x + \\log A$, which is much easier to work with.\n",
    "\n",
    "However, it is not always possible to find a form in which our data is fit well by a straight line.\n",
    "We might imagine that there are more suitable functions that fit our data better.\n",
    "For example, if we were measuring the voltage of mains AC as a function of time, this is likely to be fit well by a sinusoid $V(t) = A \\sin(100\\pi t + \\varphi)$, where we can tune the amplitude $A$ and the phase shift $\\varphi$ (the frequency we know is 50Hz) to get the best fit.\n",
    "In contrast, there is no way we can tune the gradient $m$ and the y-intercept $c$ to make the straight line $V(t) = m t + c$ fit the data well!\n",
    "\n",
    "But drawing a general function by hand is very difficult!\n",
    "How do we *actually do the fit*?\n",
    "This is a very complicated question, but all you need to understand at this point is that there are *algorithms* that *automatically* tune the *free parameters* of your function so that it fits the data as well as possible.\n",
    "\n",
    "The most important of these is called the **method of least squares**.\n",
    "The way this algorithm works is by repeatedly 'nudging' the free parameters so that each nudge results in a reduction in a quantity which encapsulates how well the function fits the data.\n",
    "For the curious, this quantity is result of adding up the *squared differences* between each data point and the function that you are trying to fit.\n",
    "This is the square of the residuals that we've been plotting.\n",
    "\n",
    "However, with such power comes, as always, great responsibility.\n",
    "The famous mathematician John von Neumann once said\n",
    "\n",
    "> *\"With four free parameters I can fit an elephant, and with five I can make him wiggle his trunk!\"*\n",
    "\n",
    "What this means is, you could come up with an extremely complicated function, with many many free parameters, and the least squares algorithm could twiddle these parameters so that the function passed through all of the error bars on your data points (which could be in the shape of an elephant!)\n",
    "This *does not mean that your function is a good representation of the underlying laws that generated the data*.\n",
    "\n",
    "If you're struggling to follow this, don't worry!\n",
    "Just bear it in mind as you continue your degree:\n",
    "**If two functions fit your data equally well, and one has fewer free parameters than the other, then you should choose the one with fewer parameters.**\n",
    "This is a version of **Occam's razor**, which basically says that \"the simplest theory that accurately predicts the phenomena is usually the correct one\".\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dated-resolution",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "continuing-atlanta",
   "metadata": {},
   "source": [
    "# Removed\n",
    "## Using a linear fit to locate the percolation transition\n",
    "\n",
    "Next, you will need to try to estimate the mid-point of the percolation transition (i.e. where the probability of percolating is 0.5) by plotting a (straight) line of best fit.\n",
    "\n",
    "You will need to choose an appropriate range of values of `frozen_prob` so that it makes sense to fit a straight line to the data.\n",
    "\n",
    "You can draw the plot by hand or use a plotting tool such as Excel, but be warned: Excel does not make it easy to plot error bars of different sizes!\n",
    "\n",
    "Run the cell below to generate values of $f$ that span the percolation transition.\n",
    "\n",
    "Record your results in a table, making sure to keep a note of the number of repeats and the estimate of the error.\n",
    "\n",
    "| Frozen prob ($q$) | Repeats ($N$) | Simulation results ($f$) | Error ($\\sigma_f$) |\n",
    "| --- | --- | --- | --- |\n",
    "| ... | ... | ... | ... |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affected-assault",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

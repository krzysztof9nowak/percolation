{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "designed-titanium",
   "metadata": {},
   "source": [
    "# Task 2: a more challenging percolation problem\n",
    "\n",
    "In this notebook, we are going to simulate a percolating substance that is able to propagate horizontally, as well as downwards.\n",
    "\n",
    "As a physical motivation, we might think of the percolating substance as water, and the simulation as representing water percolating through soil under gravity.\n",
    "This is actually a very relevant issue today; for example, understanding the way in which water percolates through different substances is crucial for accurate flood risk assessments, and flood mitigation strategies often aim to slow down the speed of percolation, without stopping it (which would lead to surface runoff and make the situation worse).\n",
    "\n",
    "The aim of this notebook will be to determine the value of $q$ - i.e. `frozen_prob` - at which percolation switches from becoming more than 50\\% probable, to less than 50\\% probable.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Definition:</b> We will define the <b>percolation threshold</b> $q^*$ as the value of $q$ which corresponds to a percolation probability of $p = \\frac{1}{2}$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entitled-situation",
   "metadata": {},
   "source": [
    "## Running this variation of the model\n",
    "\n",
    "### Importing the code\n",
    "\n",
    "Of course, the first thing we need to do is import the relevant bits of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chronic-creation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from p1b_percolation.lattice import SquareLattice\n",
    "from p1b_percolation.model import PercolationModel\n",
    "from p1b_percolation.scripts.parameter_scan import parameter_scan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "american-accuracy",
   "metadata": {},
   "source": [
    "### Setting up the model\n",
    "\n",
    "We're going to provide a second argument to `SquareLattice`, called `n_links`.\n",
    "In the previous notebook, `n_links` took its default value of 1, but we now want to change it to 3.\n",
    "\n",
    "Run the following cell to see the effect of changing `n_links`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "piano-hawaii",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(SquareLattice.n_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addressed-ambassador",
   "metadata": {},
   "source": [
    "The way we will create a $50\\times 50$ lattice is\n",
    "```python\n",
    "lattice = SquareLattice(50, 50, n_links=3)\n",
    "```\n",
    "\n",
    "You might wonder why we don't just write\n",
    "```python\n",
    "lattice = SquareLattice(50, 50, 3)\n",
    "```\n",
    "Python allows both, but it is usually preferable to explicitly state the name of the argument which you are providing a value for.\n",
    "This is particularly helpful when a function has many arguments, and you don't want to rely on remembering the order in which they are supposed to appear.\n",
    "\n",
    "We could have been even more explicit and written\n",
    "```python\n",
    "lattice = SquareLattice(n_rows=50, n_cols=50, n_links=3)\n",
    "```\n",
    "but it's easy to remember that the first two arguments specify the size of the lattice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processed-cookie",
   "metadata": {},
   "outputs": [],
   "source": [
    "lattice = SquareLattice(50, 50, n_links=3)\n",
    "model = PercolationModel(lattice, frozen_prob=0.38)\n",
    "\n",
    "model.animate(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "labeled-queue",
   "metadata": {},
   "source": [
    "## The percolation transition\n",
    "\n",
    "To estimate the percolation probability $p$, we will use a method similar to one you saw in the previous notebook.\n",
    "We will run a number, `repeats`, of separate simulations and record the fraction of these that percolated,\n",
    "\n",
    "$$\n",
    "f = \\frac{1}{N} \\sum_{i=1}^N b_i \\tag{Estimator}\n",
    "$$\n",
    "\n",
    "where $b_i$ are equal to either 0 (for simulations which don't percolate) or 1 (for those that do), and $N$ is the number of simulations (`repeats`).\n",
    "\n",
    "\n",
    "### Errors\n",
    "\n",
    "Just as before, $N \\times f$ still follows a binomial distribution, since it is the sum of binary variables $b_i$ (these are also called 'Bernoulli trials').\n",
    "Thus, the formula for the standard error on $f$ remains the same:\n",
    "\n",
    "$$\n",
    "\\sigma_f = \\sqrt{\\frac{p(1 - p)}{N}} \\tag{Standard error}\n",
    "$$\n",
    "\n",
    "Previously, we calculated $p$ by hand using basic probability theory.\n",
    "Unfortunately, in this (and almost every interesting) situation, we can't just calculate the answer by hand, otherwise we wouldn't bother spending all this effort estimating it using simulations!\n",
    "\n",
    "The standard approach in situations like this is to replace $p$ with its estimate, i.e. $f$.\n",
    "\n",
    "The cell below also calculates an estimate of the standard error using $f$ in place of $p$.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Lab book:</b> \n",
    "    Run the cell for `frozen_prob` equal to 0.34, 0.4, and 0.45, and for 10, 100 and 1000 repeats, and record the results in your lab book.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "failing-citation",
   "metadata": {},
   "outputs": [],
   "source": [
    "frozen_prob = 0.34  # change this value!\n",
    "repeats = 10  # and this one!\n",
    "\n",
    "lattice = SquareLattice(50, 50, n_links=3)\n",
    "model = PercolationModel(lattice, frozen_prob)\n",
    "\n",
    "model.estimate_percolation_prob(repeats, print_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-trainer",
   "metadata": {},
   "source": [
    "This cell is left intentionally blank."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sweet-assumption",
   "metadata": {},
   "source": [
    "You should see that the estimate of the standard error can be zero, when $f$ is either 0 or 1.\n",
    "Of course, it is not correct to conclude that the error is zero when $f=0$ or $f=1$; we cannot say with 100\\% certainty whether the next simulation will percolate or not.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Lab book:</b> \n",
    "    Discuss this observation in your lab book. What's going wrong? What is the effect of increasing $N$? Why?\n",
    "</div>\n",
    "\n",
    "**Hint:** This example illustrates why it is so important to distinguish between the estimator, $f$ (which is only as good as your experiment!), and the thing being estimated, $p$.\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Note for CO+TAs</b> Not sure whether to go on to say we used a different error for these points, or get them to try to avoid these points in their parameter scans. I would personally opt for the former.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "three-harvard",
   "metadata": {},
   "source": [
    "## Finding the percolation threshold using a 'fit'\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Definition:</b> A <b>fit</b> is when you take a function and carefully tune the parameters so that the resulting line matches your data as well as possible. You are probably very familiar with the \"line of best fit\", which corresponds to the function $f(x) = m x + c$, whose parameters are $m$ (the gradient) and $c$ (the y-intercept).\n",
    "</div>\n",
    "\n",
    "Interested students can take 5 minutes at this point to have a read through the section [Further reading: Fitting functions to data](#Further-reading:-Fitting-functions-to-data) before returning to this point.\n",
    "\n",
    "We will attempt to fit a **Logistic** function to our simulation data\n",
    "\n",
    "$$\n",
    "S(q) = \\frac{1}{1 + e^{\\lambda (q - q_0)}}\n",
    "$$\n",
    "\n",
    "$S(q)$ is a very simple function that smoothly transitions between the asymptotes\n",
    "\n",
    "$$\n",
    "\\lim_{q\\to-\\infty} S(q) = 1 \\qquad\\qquad\n",
    "\\lim_{q\\to\\infty} S(q) = 0\n",
    "$$\n",
    "\n",
    "and has two *parameters* (just like the line of best fit):\n",
    "* The mid-point $q_0$, such that $S(q_0) = \\frac{1}{2}$. We will use this to *estimate* the percolation threshold, $q^*$.\n",
    "* The 'steepness' $\\lambda$, which controls how quickly the transition from approximately 1 to approximately 0 occurs.\n",
    "\n",
    "\n",
    "Run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structured-insulin",
   "metadata": {},
   "outputs": [],
   "source": [
    "lattice = SquareLattice(50, 50, n_links=3)\n",
    "model = PercolationModel(lattice)\n",
    "\n",
    "parameter_scan(model, start=0.35, stop=0.45, num=50, repeats=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selective-musical",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Lab book:</b> \n",
    "Comment on the quality of the logistic curve fit.\n",
    "Do the residuals show any obvious structure, or are they uniformly distributed above and below the fit curve?\n",
    "</div>\n",
    "\n",
    "The Logistic function is the simplest function that has 'essentially the correct shape', and since we don't have any other clues (e.g. from an underlying theory), it is a good function to start with in this case.\n",
    "To emphasise the point again, it is important to realise that we do not have any fundamental reason to think $S(q)$ is going to fit the data perfectly, but we have to start somewhere!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "short-chrome",
   "metadata": {},
   "source": [
    "## Extrapolating to infinite lattice size\n",
    "\n",
    "There is one glaring issue with all the analysis we have done so far; all of our results depend on the size of the lattice!\n",
    "\n",
    "So-called **finite-volume effects** are an inevitability with simulations, since we cannot simulate an infinite system.\n",
    "However, it is possible to *extrapolate* results to the infinite limit, by looking at how the thing you're measuring varies as the size of simulations increases.\n",
    "The answers you get after extrapolating are more likely to agree with the physical world.\n",
    "\n",
    "We will attempt to extract some infinite-lattice results for the percolation threshold, $q^*(\\infty)$.\n",
    "\n",
    "You will need to run the cell below for a range of different values of `n_rows`, the number of rows (the number of columns will automatically be set equal to this).\n",
    "Let's label the number of rows as $L$.\n",
    "\n",
    "Record your results in a table, making sure to note down the associated errors on $q_0$ **with an appropriate precision**.\n",
    "Also record your values for `start` and `stop` in the table; these will need to be carefully selected for each lattice size so that almost all of the data points take values between 0 and 1, but are not 0 or 1.\n",
    "**Leave three blank columns in your table.**\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Lab book:</b> \n",
    "Record your results in a table.\n",
    "</div>\n",
    "\n",
    "For example, your table could look like this\n",
    "\n",
    "| Lattice size ($L$, `n_rows`) | Min $q$ (`start`) | Max $q$ (`stop`) | Mid point ($q_0$) | Error ($\\sigma_{q_0}$) | - | - | - |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| ... | ... | ... | ... | ... | ... | ... | ... |\n",
    "\n",
    "Keep a note of what you used for`num` and `repeats`, but since there is no need to vary them they do not require a column in the table.\n",
    "\n",
    "Aim to gather results for 5-10 different lattice sizes ranging between `n_rows = 20` and `n_rows = 200`.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Warning!</b> \n",
    "    The larger simulations will take a while to complete. There is no need to generate more than 25 data points for each lattice size, and 25 repeats should also suffice. The largest lattice size may take up to 5 minutes, so grab a cup of coffee while you wait!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hearing-review",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = 20  # change this!\n",
    "n_cols = n_rows\n",
    "\n",
    "lattice = SquareLattice(n_rows, n_cols, n_links=3)\n",
    "model = PercolationModel(lattice)\n",
    "\n",
    "parameter_scan(model, start=0.3, stop=0.5, num=35, repeats=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uniform-disease",
   "metadata": {},
   "source": [
    "You may be able to tell, by just studying the numbers in your table, that the relationship between lattice size (`n_rows`) and the mid-point of the transition is not linear.\n",
    "\n",
    "We really can't say *a priori* what the relationship is, but it's often a good next step to take logarithms to check if the relationship is a power-law.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Lab book:</b> \n",
    "In the remaining three columns of your table, calculate the logarithm of the lattice size, the logarithm of $q_0$, and the error on $\\log q_0$.\n",
    "</div>\n",
    "\n",
    "This will require you to *propagate* the error on $q_0$ through the logarithm.\n",
    "Refer to the lab manual if you need to double check how to do this.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Lab book:</b> \n",
    "Plot a graph of $\\log L$ v.s. $\\log q_0(L)$ and draw a line of best fit.\n",
    "Calculate the gradient of your line, with an estimate of its error.\n",
    "</div>\n",
    "\n",
    "Now, let's say we take the lattice size to tend towards infinity (i.e. we move right-wards along the horizontal axis).\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Lab book:</b> \n",
    "What does your graph predict as the position of the percolation transition for an infinite lattice, $q_0(\\infty)$?\n",
    "Can you explain this result?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "front-arizona",
   "metadata": {},
   "source": [
    "This cell is left intentionally blank."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hundred-pathology",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Lab book:</b> \n",
    "Discuss the limitations of this model to accurately describe water percolating through soil under gravity.\n",
    "How might you extent the model to make it more realistic?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "choice-vienna",
   "metadata": {},
   "source": [
    "# Further reading: Fitting functions to data\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Warning!</b> \n",
    "    Instead of talking of fitting <b>functions</b> to data, it is more common to speak of fitting a <b>model</b> to some observations.\n",
    "    In this situation, a model is just a construct which, through the correct choice of its parameters, purports to be able to predict the observations.\n",
    "</div>\n",
    "\n",
    "Throughout your time at school or college you may have spent a good deal of time drawing straight lines through data on paper, or perhaps even using something like Excel.\n",
    "\n",
    "*Linear fits* are exceptionally useful, and as such, if it is possible to manipulate your data into a form where it is fit well by a straight line, then this is the approach you should take.\n",
    "As an example, we frequently take the logarithm of our variables if they obey a power law $y = A x^\\alpha$, since the variables $\\log y$ and $\\log x$ are related by the *linear* equation $\\log y = \\alpha \\log x + \\log A$, which is much easier to work with.\n",
    "\n",
    "However, it is not always possible to find a form in which our data is fit well by a straight line.\n",
    "We might imagine that there are more suitable functions that fit our data better.\n",
    "For example, if we were measuring the voltage of mains AC as a function of time, this is likely to be fit well by a sinusoid $V(t) = A \\sin(100\\pi t + \\varphi)$, where we can tune the amplitude $A$ and the phase shift $\\varphi$ (the frequency we know is 50Hz) to get the best fit.\n",
    "In contrast, there is no way we can tune the gradient $m$ and the y-intercept $c$ to make the straight line $V(t) = m t + c$ fit the data well!\n",
    "\n",
    "But drawing a general function by hand is very difficult!\n",
    "How do we *actually do the fit*?\n",
    "This is a very complicated question, but all you need to understand at this point is that there are *algorithms* that *automatically* tune the *free parameters* of your function so that it fits the data as well as possible.\n",
    "\n",
    "The most important of these is called the **method of least squares**.\n",
    "The way this algorithm works is by repeatedly 'nudging' the free parameters so that each nudge results in a reduction in a quantity which encapsulates how well the function fits the data.\n",
    "For the curious, this quantity is result of adding up the *squared differences* between each data point and the function that you are trying to fit.\n",
    "This is the square of the residuals that we've been plotting.\n",
    "\n",
    "However, with such power comes, as always, great responsibility.\n",
    "The famous mathematician **John von Neumann** once said\n",
    "\n",
    "> \"With four free parameters I can fit an elephant, and with five I can make him wiggle his trunk!\"\n",
    "\n",
    "What he means is, you could come up with an extremely complicated function, with many many free parameters, and the least squares algorithm could twiddle these parameters so that the function passed through all of the error bars on your data points (which could be in the shape of an elephant!)\n",
    "This *does not mean that your function is a good representation of the underlying laws that generated the data*.\n",
    "\n",
    "If you're struggling to follow this, don't worry!\n",
    "Just bear it in mind as you continue your degree:\n",
    "**If two functions fit your data equally well, and one has fewer free parameters than the other, then you should choose the one with fewer parameters.**\n",
    "This is a version of **Occam's razor**, which basically says that \"the simplest theory that accurately predicts the phenomena is usually the correct one\".\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
